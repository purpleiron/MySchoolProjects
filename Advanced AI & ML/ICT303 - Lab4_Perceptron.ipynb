{"cells":[{"cell_type":"markdown","metadata":{"id":"ZdDimpLi2_nI"},"source":["# **ICT303 - Advanced Machine Learning and Artificial Intelligence**\n","# **Lab 4 - Perceptron and Multilayer Perceptron**\n","\n","The goal of this lab is to learn how to implement, in a concise way, Linear Regression, Classification, Perceptron and Multilayer Perceptron."]},{"cell_type":"markdown","source":["## **1. Solving Linear Regression using Optimization Algorithms**\n","\n","Recall from the previous tutorial that we  tried to implement from scratch the linear regression model. In this tutorial, we will use high level deep learning APIs (PyTorch in our case) for its implementation. Also, instead of implementing the analytical form, we will implement the optmization-based method, using optimizers such as Gradient Descent Algorithm (SGD) or Adam, which ard the most commonly used in deep learning.  \n","\n","First, we need to import the necessary packages:"],"metadata":{"id":"RNeECLiQOUgc"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from torch import nn"],"metadata":{"id":"qgVb8EB1lxFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.1. Defining the model**\n","\n","PyTorch provides a based Neural Network module called **nn** and is part of the library **Module**. Think about it as a template. Every neural network you define should extend this class and implement the necessary methods.\n","\n","In the case of linear regression, we define a class callec  **LinearRegression**, which contains\n","- The constructor **__init__**\n","- The **forward()** method - used to make a forward pass through the network. In other words, this method takes an input and evaluates the network's output for that input sample,\n","- The method **loss**, which defines the loss function,\n","- Also, we need to configure which optimization algorithm to use to estimate the parameters of the model (i.e., training). This is done using the method **configure_optimizers()**.\n","\n","Below, is the minimal implementation of the **LinearRegression** class.\n"],"metadata":{"id":"wnQjlBncnIS7"}},{"cell_type":"code","source":["class LinearRegression(nn.Module):\n","\n","    ## This is the constructor of the class\n","    def __init__(self, inputSize, outputSize, lr):\n","        super().__init__()\n","\n","        self.lr = lr # the learning rate\n","\n","        # The network is just a linear model\n","        self.net = nn.Linear(inputSize, outputSize)\n","\n","    ## The forward step\n","    def forward(self, X):\n","      # Computes the output given the input X\n","      return self.net(X)\n","\n","    ## The loss function - Here it is the Mean Square Error\n","    #  It measures the difference between the network output\n","    #  and the desired (groundtruth) output\n","    def loss(self, y_hat, y):\n","      fn = nn.MSELoss()\n","      return fn(y_hat, y)\n","\n","    ## The optimization algorithm\n","    #  Let's use the stochastic gradient descent (SGD)\n","    def configure_optimizers(self):\n","      return torch.optim.SGD(self.parameters(), self.lr)\n"],"metadata":{"id":"ot8TMZN-LCXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.2. Preparing training data**\n","\n","To train and test the model, we need data. One option is to collect real data but usually this is time consuming. Alternatively, one can use synthetic data, i.e., data simulated by computer.\n","\n","In this example, we will use synthetic data. Thus, for completness, I also included here the class **SyntheticRegressionData** that generates synthetic data:"],"metadata":{"id":"Lj1VAEwmpx4q"}},{"cell_type":"code","source":["class SyntheticRegressionData:\n","\n","  # This is the one used in previous lab\n","  def __init__(self, w, b, noise=0.01, num_train=1000):\n","    self.n = num_train   # no. of trianing samples\n","    self.w = w\n","    self.b = b\n","\n","    # generate self.n samples. Each sample is of dimension length of self.w\n","    self.X = torch.randn(len(self.w), self.n)\n","\n","    # Get a random noise\n","    noise = torch.randn(self.n, 1) * noise\n","\n","    # For each sample in X, generate its corresponding y value\n","    # using the equation above.\n","    # Note below that the function reshape has parameters (-1, 1).\n","    # This means that it will reshape the vector w into a 2D matrix where:\n","    # - the 2nd dimension has length 1\n","    # - the length of the first dimension will be autmatically set.\n","    #\n","    self.y = torch.matmul(torch.transpose(self.X, 0, 1), w.reshape((-1, 1))) + b + noise\n","\n","  def dim(self):\n","    # Return the dimension of the input data\n","    return self.X.size(dim=0)\n","\n","  def dimOutput(self):\n","    return self.y.size(dim=1)"],"metadata":{"id":"gBGuEj3up4ET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.3. Training process**\n","\n","Here, we need to update the **Trainer** that we created in the last lab. In particular, the important method of this class is the method ***fit()***, which finds the best parameters of the model given the training data. It has as input:\n","- The machine learning model **model**. In our case, this will be an instance of the class **LinearRegression**\n","- The training **data**. The data should include input samples (**data.X**) and their corresponding desired (groundtruth) output (**data.Y**). The latter is often referred to as labels. Note that in the example below, I am assuming that each column of **data.X** is a data sample (following the convention we had in the lecture).\n","\n","\n","The method **fit** will then iterate `max_epochs` times. At each iteration, it will call the method **fit_epoch**  whose role is:\n","- Go through all the input, feed them to the network and collect the computed output\n","- Compare, using the loss function, the discrepancy between the computed output and the groundtruth output\n","- Compute the gradient of the loss function and backpropagate it to all te layers of the network using the function `loss.backward()`,\n","- Update the values of the network parameters using the function `self.optimizer.step()`.\n","\n","Note that prior to the computation of the loss and gradient, we had to call the function `self.optimizer.zero_grad()`. This clears gradient buffers because we do not want any gradient from previous epoch to carry forward.\n","\n","The entire code of the class is given below. Note that we also provide here the function `fit_naive`, which is the implementation we did last week."],"metadata":{"id":"F71_QAGnpaAi"}},{"cell_type":"code","source":["class Trainer:\n","\n","  # The constructor of the class\n","  # It is used to set some basic training parameters (e.g., no. of epochs)\n","  def __init__(self, n_epochs = 3):\n","    self.max_epochs = n_epochs\n","    return\n","\n","  ## Training method: it uses iterative optimization\n","  #  This one is generic, i.e., it should work with any regresssion (linear or non-linear)\n","  def fit(self, model, data):\n","\n","    # configure the optimizer\n","    self.optimizer = model.configure_optimizers()\n","\n","    for epoch in range(self.max_epochs):\n","      self.fit_epoch()\n","\n","  def fit_epoch(self):\n","    inputs = torch.transpose(data.X, 0, 1) # this makes each row as a data sample\n","    labels = data.y\n","\n","    # Clear gradient buffers because we don't want any gradient from previous\n","    # epoch to carry forward, dont want to cummulate gradients\n","    self.optimizer.zero_grad()\n","\n","    # get output from the model, given the inputs\n","    outputs = model(inputs)\n","\n","    # get loss for the predicted output\n","    loss = model.loss(outputs, labels)\n","\n","    # get gradients w.r.t the parameters of the model\n","    loss.backward()\n","\n","    # update the parameters\n","    self.optimizer.step()\n","\n","  ## Training method - Previous implementation (previous lab)\n","  #  This is the naive implementation, included here for comparison\n","  #  Only works with nonlinear regression\n","  def fit_naive(self, model, data):\n","    # 2.1. Add 1 at the end of X\n","    ones = torch.ones(1, data.n)  # a column vector of ones\n","    X = torch.cat((data.X, ones), 0)\n","\n","    # 2.2. The solution\n","    A = torch.matmul(X, torch.transpose(X, 0, 1))\n","    # A should be of size num_train x num_train. To check it, uncomment the following\n","    # print(A.size())\n","\n","    B = torch.matmul(torch.inverse(A), X) #torch.transpose(X, 0, 1))\n","    # print(B.size())\n","\n","    w_estimated = torch.matmul(B, data.y)\n","    # print(w_estimated .size())\n","\n","    model.w = w_estimated[0:-1]    # get all the elements except the last one\n","    model.b = w_estimated[-1:]     # last element\n",""],"metadata":{"id":"50SFTmQmriLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.4. Putting all things together**\n","\n","Now that we have all classes in places, we need to write the main program that uses these classes to:\n","- Create synthetic data,\n","- Define the machine learning mdoel (Linear Regression in our case),\n","- Train the model,\n","- Evaluate its performance."],"metadata":{"id":"qnyx2h9wxdSd"}},{"cell_type":"code","source":["# 1. Generating some synthetic data\n","w = torch.tensor([2.0, 3.1])\n","b = 4.2\n","num_train = 5000\n","noise     = 0.01\n","data      = SyntheticRegressionData(w, b, noise, num_train)\n","inputSize = data.dim()\n","outputSize= data.dimOutput()\n","\n","# 2. The model\n","model = LinearRegression(inputSize, outputSize, lr=0.03)\n","\n","# 3. Training the network\n","# 3.1. Creating the trainer class\n","trainer = Trainer(n_epochs=100)\n","\n","# 3.2. Training the model\n","trainer.fit(model, data)\n","\n","# 4. Get the results\n","w_star = model.net.weight\n","b_star = model.net.bias\n","\n","# print(w_estimated)\n","print(\"Estimated W: \", w_star)\n","print(\"Estimate b: \",  b_star)\n","\n","# real values\n","print(\"Real W: \", w)\n","print(\"Real b: \", b)\n"],"metadata":{"id":"bPnfjXotp7sj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Questions:**\n","- What are the parameters of the model?\n","- What are the hyper parameters of the model?"],"metadata":{"id":"7ZcytBVayldT"}},{"cell_type":"markdown","source":["### **1.3. Exercise 1**\n","\n","Try to change the number of epochs, by starting with a small value, e.g., 2 and then increase it to 10, 50, 75 and 100. Then observe the effect on the result.\n","\n","Now, set the maximum number of epochs to 100 and vary the number of traning samples, from a small number to a large one, and observe the results."],"metadata":{"id":"3DD5v01akUbh"}},{"cell_type":"markdown","source":["## **2. Multilayer Perceptron**\n","\n","Now that we have seen how to create a neuron that implements a linear regression, it is time to scale up and build our first Multilayer Perceptron (MLP) using PyTorch.\n","\n","Implementing an MLP with PyTorch involves the following steps:\n","- Importing all dependencies\n","- Defining the neural network class, let's call it **MLP**, as a subclass of **nn.Module** in the same way we did for the linear regression model.\n","- Preparing the data set - in this example, let's use the **CIFAR-10** dataset, which consists of $60$K colour images (of size $32\\times32$) divided into $10$ classes, with $6000$ images per class. There are $50$K training images and $10$K test images.\n","- Initializing the loss function and optimizer.\n","- Defining the custom training loop, where all the magic happens.\n","\n","CIFAR-10 dataset has $10$ classes of objects (e.g., airplane, automobile, birdt, cat, etc.)."],"metadata":{"id":"Y0d04z6Ux-7V"}},{"cell_type":"markdown","source":["### **2.1. Dependencies**\n","\n","We will need to import:\n","- PyTorch.\n","- From it we import **nn**, which allows us to define a neural network module.\n","- The **DataLoader** so that we can feed data into the MLP during training. In this example, we will use the CIFAR10 dataset.\n","- transforms, which allows us to perform transformations on the data prior to feeding it to the MLP. We will discuss more about the utility of this step."],"metadata":{"id":"CH0BWBcl38bE"}},{"cell_type":"code","source":["# Importing all dependencies\n","import os # for some OS ops\n","import torch\n","from torch import nn\n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torchvision\n","\n","from torchvision.datasets import CIFAR10  # The data set that we will use\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n"],"metadata":{"id":"Ot4cBt1i4__c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.2. Defining the MLP neural network**\n","\n","The input to the network are images of size $32\\times32$ from CIFAR-10. Each pixel is represented with three values that encode the Red, Green and Blue component of the RGB color. Thus, the input to the MLP is a vector of size $32 \\times 32 \\times 3$. In this case, the input layer of the MLP will have $32 \\times 32 \\times 3 = 3072$ neurons.\n","\n","We would like the MLP to recognise each of the $10$ object classes. Thus, the output layer needs to have $10$ neurons.\n","\n","The number of intermediate layers as well as the number of neurons in each of the intermediate layers are hyper parameters that one needs to define and tune. These can be done either by experience or experimentally (try different values and pick up the best). In this example, we will use two hidden layers of size $64$ and $32$, respectively. Each of the hidden layers will be followed by a ReLU activation function."],"metadata":{"id":"ZDOQ9psL5JmW"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","  '''\n","    Multilayer Perceptron.\n","  '''\n","  def __init__(self, inputSize=32 * 32 * 3, outputSize=10, lr=0.01):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","      nn.Flatten(),\n","      nn.Linear(inputSize, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, 32),\n","      nn.ReLU(),\n","      nn.Linear(32, outputSize),\n","    )\n","    # Setting the learning rate\n","    self.lr = lr\n","\n","  ## The forward step\n","  def forward(self, X):\n","    # Computes the output given the input X\n","    return self.layers(X)\n","\n","  ## The loss function - Here, we will use Cross Entropy Loss\n","  def loss(self, y_hat, y):\n","    fn = nn.CrossEntropyLoss() # see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n","    return fn(y_hat, y)\n","\n","  ## The optimization algorithm\n","  #  Let's this time use Adam, which is the most commonly used optimizer in neural networks\n","  def configure_optimizers(self):\n","    # return torch.optim.SGD(self.parameters(), self.lr)\n","    return torch.optim.Adam(self.parameters(), self.lr)\n",""],"metadata":{"id":"6vsdCAfM9pgz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we have only changed two things changed compared to the **LinearRegression** class:\n","- the definition of the network layers in the method **__init__**, and\n","- the method **forward()**.\n","\n","Since it is a classification problem, here we use the Cross Entropy Loss.\n","\n","For the optimizer, we could also use SGD optimizer, but Adam is the most popular in deep learning."],"metadata":{"id":"brT9saFT-k_J"}},{"cell_type":"markdown","source":["### **2.3. The training loop**\n","\n","Here again, we will use. the class Trainer, which we reproduce here for clarity. Below, we discuss the necessary changes that need to be made."],"metadata":{"id":"o_bqSXn1ABAl"}},{"cell_type":"code","source":["class Trainer:\n","\n","  def __init__(self, n_epochs = 3):\n","    self.max_epochs = n_epochs\n","    return\n","\n","  # The fitting step\n","  def fit(self, model, data):\n","\n","    self.data = data\n","\n","    # configure the optimizer\n","    self.optimizer = model.configure_optimizers()\n","    self.model     = model\n","\n","    for epoch in range(self.max_epochs):\n","      self.fit_epoch()\n","\n","    print(\"Training process has finished\")\n","\n","  def fit_epoch(self):\n","\n","    current_loss = 0.0\n","\n","    # iterate over the DataLoader for training data\n","    # This iteration is over the batches\n","    # For each batch, it updates the network weeights and computes the loss\n","    for i, data in enumerate(self.data):\n","      # Get input aand its corresponding groundtruth output\n","      inputs, target = data\n","\n","      # Clear gradient buffers because we don't want any gradient from previous\n","      # epoch to carry forward, dont want to cummulate gradients\n","      self.optimizer.zero_grad()\n","\n","      # get output from the model, given the inputs\n","      outputs = self.model(inputs)\n","\n","      # get loss for the predicted output\n","      loss = self.model.loss(outputs, target)\n","\n","      # get gradients w.r.t the parameters of the model\n","      loss.backward()\n","\n","      # update the parameters (perform optimization)\n","      self.optimizer.step()\n","\n","      # Let's print some statisics\n","      current_loss += loss.item()\n","      if i % 500 == 499:\n","          print('Loss after mini-batch %5d: %.3f' %\n","                (i + 1, current_loss / 500))\n","          current_loss = 0.0"],"metadata":{"id":"yvi-N_uGAgAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.4. The main program**\n","The main program to train the neural network looks as follows:"],"metadata":{"id":"wGbKXoMFBzPx"}},{"cell_type":"code","source":["# 1. Loading the CIFAR-10 data set\n","# Transforms to apply to the data - More about this later\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# Loading the data\n","train_dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=transform)\n","\n","# If you would like to see the list of classes, uncomment the line below\n","# print(train_dataset.classes)\n","\n","batch_size = 10\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=1)\n","\n","# 2. The MLP model\n","mlp_model = MLP(lr=1e-04)\n","\n","# 3. Training the network\n","# 3.1. Creating the trainer class\n","trainer = Trainer(n_epochs=1)\n","\n","# 3.2. Training the model\n","trainer.fit(mlp_model, trainloader)\n"],"metadata":{"id":"iTxW5LFaB6uy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.5. Testing the trained model**\n","\n","Once trained, the network needs to be tested on images that the network hasn't seen during training. In other words, the test images should be different from those used for training.  \n","\n","We do this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions."],"metadata":{"id":"qF8kUYiBbjNq"}},{"cell_type":"code","source":["'''\n","# printing some info about the dataset\n","print(\"Number of points:\", dataset.shape[0])\n","print(\"Number of features:\", dataset.shape[1])\n","print(\"Features:\", dataset.columns.values)\n","print(\"Number of Unique Values\")\n","for col in dataset:\n","    print(col,\":\",len(dataset[col].unique()))\n","plt.figure(figsize=(12,8))\n","'''\n","classes = ('airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'truck')\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","testset = CIFAR10(os.getcwd(), train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n","\n","dataiter = iter(testloader)\n","images, labels = dataiter.next()  # this gets one batch\n","# print(images.shape[0]) # should be equal to batch_size\n","# print(labels)  # the labels of the images in the batch\n","\n","\n","# Let's see some images\n","imshow(torchvision.utils.make_grid(images))\n","print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(images.shape[0])))\n","\n","# Now, let's see what the network thinks these examples are\n","output = mlp_model(images)\n","estimated_labels = torch.max(output, 1).indices\n","#print(estimated_labels)\n","#print(labels)\n","\n","print('Estimated Labels: ', ' '.join(f'{classes[estimated_labels[j]]:5s}' for j in range(images.shape[0])))\n"],"metadata":{"id":"OlsN2hYDcnTs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  **3. Exercise 2**\n","\n","Change the number of layers in the network we defined above and observe how the network is performing. Also, compare the computation time, i.e., how long it takes to train when you increase the number of layers.\n","\n","Try also to increase the number of neurons in the intermediate layers of your network.\n","\n","**Question:** Does increasing, undefinitely, the number of layers and the number of neuron per layer always improves classification accuracy?"],"metadata":{"id":"Sc-15ltYbbm4"}},{"cell_type":"markdown","source":["##  **4. Exercise 3 - MNIST dataset**\n","\n","In this exercise, you are required to build a neural network (a MLP) in PyTorch and train it to recognize handwritten digits using the MNIST dataset.\n","\n","Instead of using the class CIFAR10 to load the data, you can use MNIST. Once the training set is loaded, please check:\n","- the resolution of each image,\n","- the number of images in each training set and\n","- the number of classes (can you guess how many classes the dataset will contain??).\n","\n","Then design an MLP by setting the adequate number of input and output neurons. The number of layers and the number of neurons in each hidden layer are hyperparameters, which you can play with and observe the performance."],"metadata":{"id":"tR_-10HFZKQs"}},{"cell_type":"markdown","source":["##  **5. Exercise 4 - Forward thoughts**\n","\n","What do you think about:\n","- the way we are currently evaluating the performance?\n","- the way we are choosing the different hyperparameters?\n","\n","Can you think of a better way to streamline this and make it more rigorous?"],"metadata":{"id":"V8L_nPwR3XSG"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}