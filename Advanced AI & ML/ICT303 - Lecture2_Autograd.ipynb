{"cells":[{"cell_type":"markdown","metadata":{"id":"ZdDimpLi2_nI"},"source":["# **ICT303 - Advanced Machine Learning and Artificial Intelligence**\n","# **Lecture 2 - Preliminaries**\n","\n","This notebook illustrates some examples of the concepts seen in Lecture 2 of ICT303. Most of the examples have been adapted from https://d2l.ai/chapter_preliminaries/index.html."]},{"cell_type":"markdown","source":["### **1. Automatic Differentiation**\n","\n","Here we will illustrate how to use automatic differentiation in PyTorch."],"metadata":{"id":"RNeECLiQOUgc"}},{"cell_type":"markdown","source":["#### **1.1. A simple example**\n","\n","Consider the function $y=2\\text{x}^T\\text{x}$ where $\\text{x} = (x_1, \\dots, x_n)^T$ i.e.,  a colum vector of   $n$ variables. To start, let's assign to $\\text{x}$ an initial value: \n"],"metadata":{"id":"3PdKVLn4V6dF"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.arange(4.0)  # creates a vector x of 4 elements and assign values 0, 1, 2, 3\n","x"],"metadata":{"id":"rRdyGqNO9zKB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660271596613,"user_tz":-480,"elapsed":3153,"user":{"displayName":"Hamid Laga","userId":"00325443460365190546"}},"outputId":"5823e29f-747e-46e5-9be4-abc1cfc21c49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 2., 3.])"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["Next, we will indicate that we will need to compute the gradient with respect to the variable $\\textbf{x}$."],"metadata":{"id":"8Pfdw6XZ-Nct"}},{"cell_type":"code","source":["x.requires_grad_(True)  # Alternatively, you can use `x = torch.arange(4.0, requires_grad=True)`\n","x.grad                  # The default value is None"],"metadata":{"id":"2wAMbWkJ-WAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now calculate the function and assign the result to $\\text{y}$."],"metadata":{"id":"Od8gK9Yx-p8j"}},{"cell_type":"code","source":["y = 2 * torch.dot(x, x)\n","y"],"metadata":{"id":"BZGjSw4o-xVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When you run the code above, it will display the value of $\\text{y}$ as $28$ and the name of the function that will be used to compute the gradient.\n","\n","We can now take the gradient of $\\text{y}$ with respect to $\\text{x}$ by calling its **backward** method, and can access the gradient via $\\text{x}$’s **grad** attribute:\n"],"metadata":{"id":"coNVA6fa-7Ri"}},{"cell_type":"code","source":["y.backward()    # Computes the gradient of y\n","\n","# Gradient with respect to x\n","x.grad"],"metadata":{"id":"OaYsc3mL_eiI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can vertify that the gradient computed with autograd is correct. In fact, we already know that the gradient of the function $y=2\\text{x}^\\top\\text{x}$  with respect to $\\text{x}$ should be $4\\text{x}$. Thus, we can now verify that the automatic gradient computation and the expected result are identical:"],"metadata":{"id":"7YELSSCT_09n"}},{"cell_type":"code","source":["x.grad == 4 * x  # compares whether the automatically computed gradient (i.e., x.grad) is equal to the manually computed gradient (i.e, 4*x)"],"metadata":{"id":"4c2CkTARAO-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let’s calculate another function of $\\textbf{x}$ and take its gradient. Note that PyTorch does not automatically reset the gradient buffer when we record a new gradient. Instead the new gradient is added to the already stored gradient. This behavior comes in handy when we want to optimize the sum of multiple objective functions. To reset the gradient buffer, we can call x.grad.zero() as follows:"],"metadata":{"id":"R1tfYqADAff2"}},{"cell_type":"code","source":["x.grad.zero_()  # Reset the gradient\n","y = x.sum()     # Sums the elements of the vector x\n","y.backward()    # Computes gradient with respect to x\n","x.grad"],"metadata":{"id":"9juDj01CAjR2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **1.2. Non-scalar variables**\n","If interested (not needed at this stage), please see Section 2.5.4 of https://d2l.ai/chapter_preliminaries/autograd.html."],"metadata":{"id":"EDyUcSM3A64V"}},{"cell_type":"markdown","source":["#### **1.3. Gradients and Python Control Flow**\n","So far we reviewed cases where the path from input to output was well-defined via a function such as $\\text{z} = \\text{x}^3$. Programming offers us a lot more freedom in how we compute results. For instance, we can make them depend on auxiliary variables or condition choices on intermediate results. \n","\n","One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. \n","\n","To illustrate this, consider the following code snippet where the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input $a$."],"metadata":{"id":"ElJSDK_hC7ih"}},{"cell_type":"code","source":["def f(a):\n","    b = a * 2\n","    while b.norm() < 1000:\n","        b = b * 2\n","    if b.sum() > 0:\n","        c = b\n","    else:\n","        c = 100 * b\n","    return c"],"metadata":{"id":"CeODMUbaDZ3B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, we call this function, passing in a random value as input. Since the input is a random variable, we do not know what form the computational graph will take. However, whenever we execute $f(a)$ on a specific input, we realize a specific computational graph and can subsequently run backward."],"metadata":{"id":"b5SCmX7lDcph"}},{"cell_type":"code","source":["a = torch.randn(size=(), requires_grad=True)\n","d = f(a)\n","d.backward()"],"metadata":{"id":"N-uJYwsrDm55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can access the gradient via the attribut grad of a"],"metadata":{"id":"CZHgnsrCDvxQ"}},{"cell_type":"code","source":["a.grad"],"metadata":{"id":"eojIv2y0D5Cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dynamic control flow is very common in deep learning. For instance, when processing text, the computational graph depends on the length of the input. In these cases, automatic differentiation becomes vital for statistical modeling since it is impossible to compute the gradient a priori."],"metadata":{"id":"ccruQj07ECyx"}},{"cell_type":"markdown","source":["#### **1.4. Some exercises**\n","\n","These exercises are from Section 2.5.6 of the textbook.\n","\n","***Question 1.*** After running the function for backpropagation, immediately run it again and see what happens. Why?\n","\n","***Question 2.*** Let $f(x) = sin(x)$. Plot the graph of $f$ and of its derivative. Do not exploit the fact that the derivative is $cos(x)$ but rather use automatic differentiation to get the result. Instead, use automatic derivation."],"metadata":{"id":"PXzP_oDJELVn"}},{"cell_type":"code","source":["import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import math\n","\n","#https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n","\n","x = torch.linspace(-math.pi, math.pi, steps=25, requires_grad=True)\n","y = torch.sin(x)\n","\n","# plotting y\n","#plt.plot(x.detach(), y.detach())\n","\n","# Note that the method detach creates a tensor that shares storage with tensor that does not require grad. \n","# It detaches the output from the computational graph. So no gradient will be backpropagated along this variable.\n","\n","print(y)\n"],"metadata":{"id":"hlVFYNnD4xfV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let’s compute a single-element output. When you call .backward() on a tensor with no arguments, it expects the calling tensor to contain only a single element, as is the case when computing a loss function. Then you can print the gradient with respect to $x$ and also plot its graph"],"metadata":{"id":"e_xmpei9SGc3"}},{"cell_type":"code","source":["out = y.sum()\n","out.backward()\n","\n","print(x.grad)\n","plt.plot(x.detach(), x.grad.detach())\n"],"metadata":{"id":"-lS-MtagSQpW"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ICT303 - Lecture2_Autograd - solution.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}